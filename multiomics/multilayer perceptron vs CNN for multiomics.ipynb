{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2krN-kjtdbG",
        "outputId": "0415cc52-949c-4711-e05e-c6976e41dfd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydXBv1INuocR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/Data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CulGvbDCdis"
      },
      "source": [
        "Loading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RBnGvGUs3W0",
        "outputId": "2d5ef9ed-89ba-4c2c-eac9-cc8e5befa412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cellfree_rna               37275\n",
              "microbiome                 18548\n",
              "metabolomics                3485\n",
              "plasma_somalogic            1300\n",
              "immune_system                534\n",
              "plasma_luminex                62\n",
              "serum_luminex                 62\n",
              "Training/Validation            1\n",
              "Gates ID                       1\n",
              "MRN                            1\n",
              "Study Subject ID Number        1\n",
              "Sex                            1\n",
              "sex_bin                        1\n",
              "timepoint                      1\n",
              "gestational_age                1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data again\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "with open(\"multiomics.pickle\", \"rb\") as file:\n",
        "    data_multiomics = pickle.load(file)\n",
        "\n",
        "# display the column names and their respective counts\n",
        "data_multiomics.columns.get_level_values(0).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atzihqS8CglO"
      },
      "source": [
        "Transforming the input into:\n",
        "1. Vanilla - keeps the features intact for training\n",
        "2. Individual PCA - PCA on each feature individually keeping 95% of the variance in the feature-space\n",
        "3. Whole PCA - PCA on all features combined keeping 95% of the variance in the feature-space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af6GkcbKu-6T",
        "outputId": "43ae2d02-d153-4c91-d795-2c3fa7205437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_vanilla shape: (68, 61266)\n",
            "X_individual_PCA shape: (68, 103)\n",
            "X_whole_PCA shape: (68, 51)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Input variable names\n",
        "input_variable_names = [\"cellfree_rna\", \"microbiome\", \"metabolomics\",\n",
        "                        \"plasma_somalogic\", \"immune_system\", \"plasma_luminex\", \"serum_luminex\"]\n",
        "\n",
        "# variance percentage\n",
        "variance_percentage = 0.95\n",
        "pca = PCA(n_components=variance_percentage)\n",
        "\n",
        "# Dictionary to store input variables\n",
        "inputs = {}\n",
        "\n",
        "# Dictionary to store input variables with PCA\n",
        "input_variables_pca = {}\n",
        "\n",
        "# Perform PCA for each input variable\n",
        "for variable in input_variable_names:\n",
        "    # Get the data for the variable\n",
        "    data = data_multiomics[variable]\n",
        "\n",
        "    # Store the input variable\n",
        "    inputs[variable] = data\n",
        "\n",
        "    # Perform PCA with 95% variance\n",
        "    transformed_data = pca.fit_transform(data)\n",
        "\n",
        "    # Store the PCA-transformed data\n",
        "    input_variables_pca[variable] = transformed_data\n",
        "\n",
        "# Concatenate the individual input variables\n",
        "X_vanilla = np.hstack([inputs[variable]\n",
        "                         for variable in input_variable_names])\n",
        "\n",
        "# Concatenate the individual PCA-transformed data\n",
        "X_individual_PCA = np.hstack([input_variables_pca[variable]\n",
        "                             for variable in input_variable_names])\n",
        "\n",
        "# Normalize the stacked inputs using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "normalized_inputs = scaler.fit_transform(X_vanilla)\n",
        "\n",
        "# Perform PCA to reduce dimensionality preserving 95% variance\n",
        "X_whole_PCA = pca.fit_transform(normalized_inputs)\n",
        "\n",
        "# Display the shapes of the input data\n",
        "print(\"X_vanilla shape:\", X_vanilla.shape)\n",
        "print(\"X_individual_PCA shape:\", X_individual_PCA.shape)\n",
        "print(\"X_whole_PCA shape:\", X_whole_PCA.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6o-SQbiC9Hl"
      },
      "source": [
        "CNN implementation. I started out with more factors in the hyperparameters but my Google Colab resources protested hard against how much work they would do for free. So I had to make some tough calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qReZpt4mdY9",
        "outputId": "b98b0404-8068-4434-d553-f8365ac6959f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Grid Search Progress: 100%|██████████| 192/192 [1:45:50<00:00, 33.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Combination: (0.1, 32, 32, 7, 'avg', 0.1, 0.01, 'adam', 'fixed', 4, 'whole_PCA')\n",
            "Best Accuracy: 0.7392857142857144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, hidden_size, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x.squeeze(2))\n",
        "        out = self.relu(out)\n",
        "        out = torch.mean(out, dim=2)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Set the number of bootstrap iterations\n",
        "n_bootstrap_iterations = 100\n",
        "\n",
        "# Prepare the data\n",
        "# the X's have been already generated in the previous step\n",
        "y = data_multiomics[\"Sex\"].map({\"Female\": 0, \"Male\": 1}).to_numpy()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_vanilla = torch.Tensor(X_vanilla)\n",
        "X_individual_PCA = torch.Tensor(X_individual_PCA)\n",
        "X_whole_PCA = torch.Tensor(X_whole_PCA)\n",
        "y = torch.Tensor(y)\n",
        "\n",
        "# Define the hyperparameters and their values for grid search\n",
        "hyperparameters = {\n",
        "    'learning_rate': [0.1],\n",
        "    'hidden_size': [32],\n",
        "    'batch_size': [16, 32],\n",
        "    'kernel_size': [3, 7],\n",
        "    'pooling': ['max', 'avg'],\n",
        "    'dropout_rate': [0.1],\n",
        "    'weight_decay': [0.01],\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'learning_rate_schedule': ['fixed', 'step_decay'],\n",
        "    'network_depth': [2, 4],\n",
        "    'X_choice': ['vanilla', 'individual_PCA', 'whole_PCA']\n",
        "}\n",
        "\n",
        "# Generate all possible combinations of hyperparameters\n",
        "combinations = list(itertools.product(*hyperparameters.values()))\n",
        "\n",
        "# Initialize variables to store the best combination and accuracy\n",
        "best_combination = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Check if CUDA is available and move the model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Iterate over each combination\n",
        "for combination in tqdm(combinations, desc=\"Grid Search Progress\"):\n",
        "    learning_rate, hidden_size, batch_size, kernel_size, pooling, dropout_rate, weight_decay, optimizer, learning_rate_schedule, network_depth, X_choice = combination\n",
        "\n",
        "    # Select the appropriate X based on the choice\n",
        "    if X_choice == 'vanilla':\n",
        "        X = X_vanilla\n",
        "    elif X_choice == 'individual_PCA':\n",
        "        X = X_individual_PCA\n",
        "    elif X_choice == 'whole_PCA':\n",
        "        X = X_whole_PCA\n",
        "\n",
        "    # Create an instance of the CNN model\n",
        "    model = CNN(X.shape[1], hidden_size, 2).to(device)\n",
        "\n",
        "    # Define the optimizer and loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if optimizer == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Perform bootstrapping\n",
        "    accuracy_scores = []\n",
        "    for _ in range(n_bootstrap_iterations):\n",
        "        # Resample the data\n",
        "        X_boot, y_boot = resample(X, y)\n",
        "\n",
        "        # Split the data into train and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_boot, y_boot, test_size=0.2, stratify=y_boot)\n",
        "\n",
        "        # Create PyTorch datasets and data loaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 100\n",
        "        early_stopping_epochs = 10\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs.unsqueeze(1))\n",
        "                loss = criterion(outputs, labels.long())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in test_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs.unsqueeze(1))\n",
        "                    val_loss += criterion(outputs, labels.long()).item()\n",
        "            val_loss /= len(test_loader)\n",
        "\n",
        "            # Check for early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "                if early_stopping_counter >= early_stopping_epochs:\n",
        "                    break\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs.unsqueeze(1))\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.long()).sum().item()\n",
        "        accuracy = correct / total\n",
        "        accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Compute the mean accuracy for the current combination\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "    # Check if the current combination has higher accuracy\n",
        "    if mean_accuracy > best_accuracy:\n",
        "        best_accuracy = mean_accuracy\n",
        "        best_combination = combination\n",
        "\n",
        "# Print the best combination and accuracy\n",
        "print(\"Best Combination:\", best_combination)\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuN91I9i_OoZ"
      },
      "source": [
        "And for posterity's sake, I also finetuned the multilayer perceptron approach here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1wAe0YjvL8",
        "outputId": "d916d30c-9f6d-4cf4-bd1d-0128c1b78666"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Grid Search:   0%|          | 0/48 [01:03<?, ?it/s]\n",
            "Grid Search: 100%|██████████| 48/48 [00:34<00:00,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters:\n",
            "{'input_dim': 51, 'hidden_dim': 32, 'output_dim': 2, 'learning_rate': 0.1, 'num_epochs': 100, 'batch_size': 16, 'dropout_rate': 0.1, 'weight_decay': 0.01, 'optimizer': 'adam', 'learning_rate_schedule': 'fixed', 'network_depth': 2}\n",
            "Best Loss: 0.0000\n",
            "Best Accuracy: 85.71%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "class ExperimentConfig:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate, num_epochs, batch_size):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "def run_experiment(config, X, y):\n",
        "    # Set random seeds for reproducibility and stable results\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Encode the target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.Tensor(X_train)\n",
        "    y_train_tensor = torch.Tensor(y_train).long()\n",
        "    X_test_tensor = torch.Tensor(X_test)\n",
        "    y_test_tensor = torch.Tensor(y_test).long()\n",
        "\n",
        "    # Create a PyTorch DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    # Define the MLP model\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "            super(MLP, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    # Create an instance of the MLP model\n",
        "    model = MLP(config.input_dim, config.hidden_dim, config.output_dim)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_accuracy = 0.0\n",
        "    patience = 5\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            counter = 0\n",
        "            # Evaluate the model on the test set\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                X_test_tensor = X_test_tensor.to(device)\n",
        "                y_test_tensor = y_test_tensor.to(device)\n",
        "                outputs = model(X_test_tensor)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "                best_accuracy = accuracy\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                break\n",
        "\n",
        "    return best_loss, best_accuracy\n",
        "\n",
        "\n",
        "# Define the hyperparameters and their values\n",
        "hyperparameters = {\n",
        "    'learning_rate': [0.1],\n",
        "    'hidden_size': [32],\n",
        "    'batch_size': [16, 32],\n",
        "    'dropout_rate': [0.1],\n",
        "    'weight_decay': [0.01],\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'learning_rate_schedule': ['fixed', 'step_decay'],\n",
        "    'network_depth': [2, 4],\n",
        "    'X_choice': ['vanilla', 'individual_PCA', 'whole_PCA']\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "best_loss = float('inf')\n",
        "best_accuracy = 0.0\n",
        "best_hyperparameters = None\n",
        "\n",
        "total_combinations = np.prod([len(values) for values in hyperparameters.values()])\n",
        "pbar = tqdm(total=total_combinations, desc=\"Grid Search\")\n",
        "for values in itertools.product(*hyperparameters.values()):\n",
        "    if values[8] == 'vanilla':\n",
        "        X = X_vanilla\n",
        "    elif values[8] == 'individual_PCA':\n",
        "        X = X_individual_PCA\n",
        "    elif values[8] == 'whole_PCA':\n",
        "        X = X_whole_PCA\n",
        "\n",
        "    config = ExperimentConfig(\n",
        "        input_dim=X.shape[1],\n",
        "        hidden_dim=values[1],\n",
        "        output_dim=2,\n",
        "        learning_rate=values[0],\n",
        "        num_epochs=100,\n",
        "        batch_size=values[2]\n",
        "    )\n",
        "\n",
        "    # init target\n",
        "    y = data_multiomics[\"Sex\"].map({\"Female\": 0, \"Male\": 1}).to_numpy()\n",
        "\n",
        "    config.dropout_rate = values[3]\n",
        "    config.weight_decay = values[4]\n",
        "    config.optimizer = values[5]\n",
        "    config.learning_rate_schedule = values[6]\n",
        "    config.network_depth = values[7]\n",
        "\n",
        "    loss, accuracy = run_experiment(config, X, y)\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = config\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Print the best hyperparameters, loss, and accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_hyperparameters.__dict__)\n",
        "print(\"Best Loss: {:.4f}\".format(best_loss))\n",
        "print(\"Best Accuracy: {:.2%}\".format(best_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPWKskNN_ZKm"
      },
      "source": [
        "Which scores higher. Seems like my assumption that a CNN would do well doesn't hold up under these assumptions.\n",
        "\n",
        "Additionally, and more importantly, I reimplemented the models from the \"Sex\" prediction task which was a binary classification task and the classes were equally distributed. So Accuracy made sense. However, for this one, i.e. \"gestational age\", it doesn't really make sense because the classes were imbalanced (which is why I was thinking of using stratified bootstrapping. So F1 scores would have made more sense for training and validation.\n",
        "\n",
        "Point to note: The PCA on the whole thing combined was the best scoring input selection which might be because the networks are relatively shallow. Although I'm not certain how I'd train deep networks for tasks like these with so many features. Not to mention the problems sparsity would bring to the table.\n",
        "\n",
        "Finally for now, I'd have liked to work out the relative importance of the features, but I'm running into serious compute limitations on Colab. So I'll leave the current experimentation with the statement:\n",
        "\"I have a truly marvelous demonstration of this proposition that this margin is too narrow to contain.\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
